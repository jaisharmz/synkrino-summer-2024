{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "airTvG3VSAm2",
        "outputId": "e9da13bf-f872-4d90-da09-9be17befa57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: soupsieve, beautifulsoup4\n",
            "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.5\n",
            "Collecting google\n",
            "  Downloading google-3.0.0-py2.py3-none-any.whl.metadata (627 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in ./.conda/lib/python3.11/site-packages (from google) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in ./.conda/lib/python3.11/site-packages (from beautifulsoup4->google) (2.5)\n",
            "Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google\n",
            "Successfully installed google-3.0.0\n"
          ]
        }
      ],
      "source": [
        "# ! pip install -q streamlit\n",
        "# ! pip install langchain\n",
        "# ! pip install langchain-openai\n",
        "# ! pip install -qU langchain-text-splitters\n",
        "# ! pip install langchain-chroma\n",
        "# ! pip install lxml\n",
        "! pip install beautifulsoup4\n",
        "! pip install google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MFBqSPDxQF3E"
      },
      "outputs": [],
      "source": [
        "# %pip install -qU langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6sSQHL9WQXbe"
      },
      "outputs": [],
      "source": [
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
        "\n",
        "# # List of URLs\n",
        "# urls = [\n",
        "#     \"https://grants.nih.gov/grants/guide/pa-files/par-19-357.html\",\n",
        "#     \"https://grants.nih.gov/grants/guide/pa-files/PAR-21-135.html\",\n",
        "#     \"https://grants.nih.gov/grants/guide/rfa-files/RFA-AG-15-014.html\",\n",
        "#     \"https://grants.nih.gov/grants/guide/pa-files/PAR-21-130.html\",\n",
        "#     \"https://grants.nih.gov/grants/guide/pa-files/PAR-18-026.html\",\n",
        "# ]\n",
        "\n",
        "# headers_to_split_on = [\n",
        "#     (\"h1\", \"Header 1\"),\n",
        "#     (\"h2\", \"Header 2\"),\n",
        "#     (\"h3\", \"Header 3\"),\n",
        "#     (\"h4\", \"Header 4\"),\n",
        "# ]\n",
        "\n",
        "# # Initialize HTML header text splitter\n",
        "# html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# # Splitting parameters\n",
        "# chunk_size = 500\n",
        "# chunk_overlap = 30\n",
        "\n",
        "# # Initialize recursive character text splitter\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "# )\n",
        "\n",
        "# all_splits = []\n",
        "\n",
        "# # Process each URL\n",
        "# for url in urls:\n",
        "#     # Split HTML content from URL (based on headers)\n",
        "#     html_header_splits = html_splitter.split_text_from_url(url)\n",
        "#     # Split HTML content into chunks (based on chunk size and overlap)\n",
        "#     splits = text_splitter.split_documents(html_header_splits)\n",
        "#     # Append to all splits\n",
        "#     all_splits.extend(splits)\n",
        "\n",
        "# # Display a sample of the splits\n",
        "# print(all_splits[80:85])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GwUUN_H0TThO"
      },
      "outputs": [],
      "source": [
        "# %%writefile app.py\n",
        "\n",
        "# import streamlit as st\n",
        "\n",
        "# st.write('Hello, *World!* :sunglasses:')\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
        "\n",
        "# # List of URLs\n",
        "# urls = [\n",
        "#     \"https://grants.nih.gov/grants/guide/pa-files/par-19-357.html\",\n",
        "#     \"https://grants.nih.gov/grants/guide/pa-files/PAR-21-135.html\",\n",
        "#     \"https://grants.nih.gov/grants/guide/rfa-files/RFA-AG-15-014.html\",\n",
        "#     \"https://grants.nih.gov/grants/guide/pa-files/PAR-21-130.html\",\n",
        "#     \"https://grants.nih.gov/grants/guide/pa-files/PAR-18-026.html\",\n",
        "# ]\n",
        "\n",
        "# headers_to_split_on = [\n",
        "#     (\"h1\", \"Header 1\"),\n",
        "#     (\"h2\", \"Header 2\"),\n",
        "#     (\"h3\", \"Header 3\"),\n",
        "#     (\"h4\", \"Header 4\"),\n",
        "# ]\n",
        "\n",
        "# # Initialize HTML header text splitter\n",
        "# html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# # Splitting parameters\n",
        "# chunk_size = 500\n",
        "# chunk_overlap = 30\n",
        "\n",
        "# # Initialize recursive character text splitter\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "# )\n",
        "\n",
        "# all_splits = []\n",
        "\n",
        "# # Process each URL\n",
        "# for url in urls:\n",
        "#     # Split HTML content from URL (based on headers)\n",
        "#     html_header_splits = html_splitter.split_text_from_url(url)\n",
        "#     # Split HTML content into chunks (based on chunk size and overlap)\n",
        "#     splits = text_splitter.split_documents(html_header_splits)\n",
        "#     # Append to all splits\n",
        "#     all_splits.extend(splits)\n",
        "\n",
        "# # Display a sample of the splits\n",
        "# st.write(all_splits[80:85])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxyW8d0DMr8y",
        "outputId": "d78a903f-d721-4109-b86b-7dcf3aba5cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "\n",
        "# os.system(\"pip install langchain\")\n",
        "# os.system(\"pip install langchain-openai\")\n",
        "# os.system(\"pip install -qU langchain-text-splitters\")\n",
        "# os.system(\"pip install langchain-chroma\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"apikey\"\n",
        "\n",
        "import streamlit as st # Import python packages\n",
        "# from snowflake.snowpark.context import get_active_session\n",
        "# session = get_active_session() # Get the current credentials\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"max_colwidth\",None)\n",
        "\n",
        "### Default Values\n",
        "#model_name = 'mistral-7b' #Default but we allow user to select one\n",
        "num_chunks = 3 # Num-chunks provided as context. Play with this to check how it affects your accuracy\n",
        "slide_window = 7 # how many last conversations to remember. This is the slide window.\n",
        "debug = 1 #Set this to 1 if you want to see what is the text created as summary and sent to get chunks\n",
        "# use_chat_history = 0 #Use the chat history by default\n",
        "\n",
        "### Functions\n",
        "def main():\n",
        "    st.title(f\":speech_balloon: Biology Chat Assistant (Synkrino)\")\n",
        "    urls_input = ask_urls()\n",
        "\n",
        "    urls_input = urls_input.split(\"\\n\")\n",
        "    urls_input = [i for i in urls_input if \"http\" in i]\n",
        "\n",
        "    st.write(\"List of documents:\")\n",
        "\n",
        "    initialize(urls_input)\n",
        "\n",
        "    st.dataframe(urls)\n",
        "\n",
        "    config_options()\n",
        "    init_messages()\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    if question := st.chat_input(\"Prompt here...\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(question)\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            message_placeholder = st.empty()\n",
        "\n",
        "            question = question.replace(\"'\",\"\")\n",
        "\n",
        "            with st.spinner(f\"{st.session_state.model_name} thinking...\"):\n",
        "                response = complete(question)\n",
        "                message_placeholder.markdown(response)\n",
        "\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "def ask_urls():\n",
        "    global valid\n",
        "    valid = False\n",
        "    urls_input = st.text_area(\"URLs to use:\",\n",
        "\"\"\"https://grants.nih.gov/grants/guide/pa-files/par-19-357.html\n",
        "https://grants.nih.gov/grants/guide/pa-files/PAR-21-135.html\n",
        "https://grants.nih.gov/grants/guide/rfa-files/RFA-AG-15-014.html\n",
        "https://grants.nih.gov/grants/guide/pa-files/PAR-21-130.html\n",
        "https://grants.nih.gov/grants/guide/pa-files/PAR-18-026.html\"\"\"\n",
        "                  )\n",
        "    return urls_input\n",
        "\n",
        "def initialize(urls_input=None):\n",
        "    global urls, db, llm # Should change this later\n",
        "\n",
        "    # List of URLs\n",
        "    if urls_input is None:\n",
        "        urls = [\n",
        "            \"https://grants.nih.gov/grants/guide/pa-files/par-19-357.html\",\n",
        "            \"https://grants.nih.gov/grants/guide/pa-files/PAR-21-135.html\",\n",
        "            \"https://grants.nih.gov/grants/guide/rfa-files/RFA-AG-15-014.html\",\n",
        "            \"https://grants.nih.gov/grants/guide/pa-files/PAR-21-130.html\",\n",
        "            \"https://grants.nih.gov/grants/guide/pa-files/PAR-18-026.html\",\n",
        "        ]\n",
        "    else:\n",
        "        urls = urls_input\n",
        "\n",
        "    headers_to_split_on = [\n",
        "        (\"h1\", \"Header 1\"),\n",
        "        (\"h2\", \"Header 2\"),\n",
        "        (\"h3\", \"Header 3\"),\n",
        "        (\"h4\", \"Header 4\"),\n",
        "    ]\n",
        "\n",
        "    # Initialize HTML header text splitter\n",
        "    html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "    # Splitting parameters\n",
        "    chunk_size = 500\n",
        "    chunk_overlap = 30\n",
        "\n",
        "    # Initialize recursive character text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    # Create splits\n",
        "    all_splits = []\n",
        "    for url in urls:\n",
        "        html_header_splits = html_splitter.split_text_from_url(url)\n",
        "        splits = text_splitter.split_documents(html_header_splits)\n",
        "        all_splits.extend(splits)\n",
        "\n",
        "    # Create vector store and LLM\n",
        "    db = Chroma.from_documents(all_splits, OpenAIEmbeddings())\n",
        "    llm = ChatOpenAI(temperature=0.5, model_name=\"gpt-3.5-turbo\", max_tokens=512)\n",
        "\n",
        "def config_options():\n",
        "    st.sidebar.selectbox('Select your model:',(\n",
        "                                    'mixtral-8x7b',\n",
        "                                    'snowflake-arctic',\n",
        "                                    'mistral-large',\n",
        "                                    'llama3-8b',\n",
        "                                    'llama3-70b',\n",
        "                                    'reka-flash',\n",
        "                                     'mistral-7b',\n",
        "                                     'llama2-70b-chat',\n",
        "                                     'gemma-7b'), key=\"model_name\")\n",
        "\n",
        "    # For educational purposes. Users can chech the difference when using memory or not\n",
        "    st.sidebar.checkbox('Do you want that I remember the chat history?', key=\"use_chat_history\", value = True)\n",
        "\n",
        "    st.sidebar.checkbox('Debug: Click to see summary generated of previous conversation', key=\"debug\", value = True)\n",
        "    st.sidebar.button(\"Start Over\", key=\"clear_conversation\")\n",
        "    st.sidebar.expander(\"Session State\").write(st.session_state)\n",
        "\n",
        "\n",
        "def init_messages():\n",
        "    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "def get_similar_chunks (question):\n",
        "    similar_chunks = db.similarity_search(question, k=num_chunks)\n",
        "    return similar_chunks\n",
        "\n",
        "def get_chat_history():\n",
        "    chat_history = []\n",
        "    start_index = max(0, len(st.session_state.messages) - slide_window)\n",
        "    for i in range (start_index , len(st.session_state.messages) -1):\n",
        "         chat_history.append(st.session_state.messages[i])\n",
        "    return chat_history\n",
        "\n",
        "def summarize_question_with_history(chat_history, question):\n",
        "    prompt = f\"\"\"\n",
        "        Based on the chat history below and the question, generate a query that extend the question\n",
        "        with the chat history provided. The query should be in natual language.\n",
        "        Answer with only the query. Do not add any explanation.\n",
        "\n",
        "        <chat_history>\n",
        "        {chat_history}\n",
        "        </chat_history>\n",
        "        <question>\n",
        "        {question}\n",
        "        </question>\n",
        "        \"\"\"\n",
        "    result = complete_llm(prompt)\n",
        "    if st.session_state.debug:\n",
        "        st.sidebar.text(\"Summary to be used to find similar chunks in the docs:\")\n",
        "        st.sidebar.caption(result)\n",
        "    return result\n",
        "\n",
        "def create_prompt (myquestion):\n",
        "\n",
        "    if st.session_state.use_chat_history:\n",
        "        chat_history = get_chat_history()\n",
        "\n",
        "        if chat_history != []: #There is chat_history, so not first question\n",
        "            question_summary = summarize_question_with_history(chat_history, myquestion)\n",
        "            prompt_context =  get_similar_chunks(question_summary)\n",
        "        else:\n",
        "            prompt_context = get_similar_chunks(myquestion) #First question when using history\n",
        "    else:\n",
        "        prompt_context = get_similar_chunks(myquestion)\n",
        "        chat_history = \"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "           You are an expert chat assistance that extracs information from the CONTEXT provided\n",
        "           between <context> and </context> tags.\n",
        "           You offer a chat experience considering the information included in the CHAT HISTORY\n",
        "           provided between <chat_history> and </chat_history> tags..\n",
        "           When ansering the question contained between <question> and </question> tags\n",
        "           be concise and do not hallucinate.\n",
        "           If you don´t have the information just say so.\n",
        "\n",
        "           Do not mention the CONTEXT used in your answer.\n",
        "           Do not mention the CHAT HISTORY used in your asnwer.\n",
        "\n",
        "           <chat_history>\n",
        "           {chat_history}\n",
        "           </chat_history>\n",
        "           <context>\n",
        "           {prompt_context}\n",
        "           </context>\n",
        "           <question>\n",
        "           {myquestion}\n",
        "           </question>\n",
        "           Answer:\n",
        "           \"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def complete(myquestion):\n",
        "    prompt = create_prompt(myquestion)\n",
        "    result = complete_llm(prompt)\n",
        "    return result\n",
        "\n",
        "def complete_llm(prompt, system_prompt=None):\n",
        "    if system_prompt is None:\n",
        "        system_prompt = \"You are a helpful assistant that gives information about biology.\"\n",
        "    messages = [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", prompt),\n",
        "    ]\n",
        "    result = llm.invoke(messages).content\n",
        "    result = result.replace(\"'\", \"\")\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-6TMs7YKTkLK"
      },
      "outputs": [],
      "source": [
        "# ! streamlit run app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
